# 特征向量、特征值

要理解矩阵的特征向量和特征值，需要对线性变换、行列式、线性方程组、基变换等内容有一定了解。

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jpe4ek9j20x40i8whw.jpg" alt="image-20220613111157721" style="width: 540px;" />

## 什么是特征向量、特征值

对于线性变换 $A=\begin{bmatrix}3&1\\0&2\end{bmatrix}$，变换后大部分向量都离开了原来所在的直线( 或者说向量张成的空间 )，但也有一些特殊向量仍保持着方向，只是被拉伸或压缩了 $n$ 倍。

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36wiytlzlg20k00b91ky.gif" alt="Kapture 2022-06-13 at 15.39.37" style="zoom:75%;" />

这些特殊向量被称为变换的”特征向量“，特征向量被拉伸或压缩的比例称为这个特征向量的特征值。

从图中可以看出，如果一个向量是矩阵的特征向量，那么它所张成的空间（向量所在直线）中所有的向量都是特征向量，它们都会伸缩相同的倍数。

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jpg6cz8g20k00batoe.gif" alt="Kapture 2022-06-13 at 11.14.55" style="zoom:75%;" />

特征向量、特征值有什么用？这里举两点：

* 我们平常理解一个线性变换，是用变换后的基向量作为矩阵的列来表示的，但这依赖于特定的坐标系，因此更好的方法是求出它的特征向量和特征值。

* 对于三维旋转变换，变换矩阵的特征向量就是旋转轴，因为只有旋转轴会在旋转前后不改变方向，且特征向量的特征值为 1，毕竟旋转不会缩放任何向量。

  <img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36oekfb1og20k00bawi3.gif" alt="Kapture 2022-06-13 at 15.53.11" style="zoom:75%;" />

## 特征值和特征向量的计算

从特征向量的定义来看，不难得出：
$$
A\vec{v}=\lambda\vec{v}
$$
其中 $\vec{v}$ 是特征向量，$\lambda$ 是特征值。这个公式表明特征向量 $\vec{v}$ 在经过线性变换 $A$ 后被伸缩了 $\lambda$ 倍。

由此我们可以推出：
$$
(A-\lambda I)\vec{v}=\vec{0}
$$
<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jpz3lktg20k00b9gu4.gif" alt="Kapture 2022-06-13 at 12.32.14" style="zoom:75%;" />

$A-\lambda I$ 是一个变换矩阵，$\vec{v}$ 是一个向量。要想得到零向量，根据行列式的内容，我们需要变换矩阵将空间降维，也就是说行列式为 0。
$$
det(A-\lambda I)=0
$$
<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jq77q4cg20k00b9qv6.gif" alt="Kapture 2022-06-13 at 12.32.14" style="zoom:75%;" />

整理一下前面的推导过程：

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jqbeh64j20s00e0q38.jpg" alt="image-20220613125339042" style="width: 540px;" />

现在我们知道，对于一个变换 $A$，要求它的特征向量和特征值，需要 $det(A-\lambda I)=0$。

当存在 $\lambda$ ，满足 $det(A-\lambda I)=0$， $\lambda$ 就是特征值。

如果 $A=\begin{bmatrix}3&1\\0&2\end{bmatrix}$，求得 $\lambda=3,\lambda=2$。

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36jqg0rnhg20k00b9avb.gif" alt="Kapture 2022-06-13 at 12.32.14" style="zoom:75%;" />

分别将这两个特征值代入，可求得：
$$
\lambda=2 \quad\Rightarrow\quad
\begin{bmatrix}1&1\\0&0\end{bmatrix}
\begin{bmatrix}x\\y\end{bmatrix}=
\begin{bmatrix}0\\0\end{bmatrix} \quad\Rightarrow\quad
x+y=0
$$

$$
\lambda=3 \quad\Rightarrow\quad
\begin{bmatrix}0&1\\0&-1\end{bmatrix}
\begin{bmatrix}x\\y\end{bmatrix}=
\begin{bmatrix}0\\0\end{bmatrix} \quad\Rightarrow\quad
y=0
$$

也就是说，$y=-x$ 和 $y=0$ ( $x$ 轴 ) 这两条线上的向量都是特征向量，特征值分别为 2 和 3。

一个矩阵不一定存在特征向量。比如逆时针旋转 90 度的变换矩阵为 $\begin{bmatrix}0&-1\\1&0\end{bmatrix}$。

同样地，求 $det(A-\lambda I)=0$ 时的 $\lambda$ 值。
$$
det\Bigg(\begin{bmatrix}-\lambda&-1\\1&-\lambda\end{bmatrix}\Bigg)
\quad\Rightarrow\quad
\lambda^2+1=0
\quad\Rightarrow\quad
\lambda=i,\lambda=-i
$$
**没有实数解说明矩阵没有特征向量**。

## 特征基

如果一个变换将所有向量拉伸 2 倍，比如 $A=\begin{bmatrix}2&0\\0&2\end{bmatrix}$，那么所有的向量都是特征向量，且特征值为 2。

如果基向量恰好是特征向量，那么变换矩阵一定是对角矩阵( 除了对角线其余值都为 0 的矩阵 )。相反的，如果一个矩阵是对角矩阵，那么它所有的基向量都是特征向量。且对角线上的值恰好是每个特征向量的特征值。

举个例子：变换 $A=\begin{bmatrix}-1&0\\0&2\end{bmatrix}$ 使 $x$ 轴翻转，$y$ 轴扩大两倍，$A$ 的对角线恰好是特征值 $\lambda=-1,\lambda=2$。

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36v8ss3u5g20k00ba4id.gif" alt="Kapture 2022-06-13 at 19.56.28" style="zoom:75%;" />

### 用特征基简化计算

对于矩阵 $A=\begin{bmatrix}3&1\\0&2\end{bmatrix}$，假设要求它的 100 次幂，100 次矩阵相乘的计算量太大。但是如果能转换为对角矩阵就很容易计算了。比如：
$$
\begin{bmatrix}x&0\\0&y\end{bmatrix}^n=\begin{bmatrix}x^n&0\\0&y^n\end{bmatrix}
$$
前面提到，如果一个矩阵的特征向量能作为基向量，那么一定是个对角矩阵。

因此，我们可以用基变换，用足以张成整个空间的特征向量作为新的基向量，计算完后再换回来。

对于 $A=\begin{bmatrix}3&1\\0&2\end{bmatrix}$ 来说，它有能张成全空间的特征向量：

<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36xskksdgg20k00b9n1o.gif" alt="Kapture 2022-06-13 at 21.07.18" style="zoom:75%;" />

取 $\vec{v}=(1,0),\vec{w}=(-1,1)$ 作为新的基向量，还记得基变换一节中的公式吗
$$
M'=A^{-1}MA
$$
<img class="img-shadow" src="https://raw.githubusercontent.com/yamsfeer/pic-bed/master/e6c9d24egy1h36xxbfi8bj20x60imgny.jpg" alt="image-20220613213210154" style="width: 540px;" />

基变换后的矩阵必然是个对角矩阵，要进行 100 次幂的计算很容易。

## 总结

一个矩阵的特征向量在变换后方向不变，拉伸或压缩一定倍数，伸缩的倍数就是这个特征向量的特征值。

要求一个矩阵 $A$ 的特征值，需要满足 $det(A-\lambda I)=0$ ，$\lambda$ 就是特征值。

将 $\lambda$ 代入方程 $(A-\lambda I)\vec{v}=\vec{0}$ 可求得特征向量。如果方程不存在实数解，说明该矩阵不存在特征向量。

如果基向量恰好是特征向量，那么变换矩阵一定是对角矩阵，矩阵对角元就是特征值。

当一个变换的特征向量足够多（能张成全空间），用特征向量作为新的基向量进行基变换，可以转化为对角矩阵，对角矩阵可以简化矩阵计算。
